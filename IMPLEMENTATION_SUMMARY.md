# LiveTalking é¡¹ç›®æ”¹åŠ¨å®æ–½æ€»ç»“

## ğŸ¯ å®Œæˆçš„ä¸»è¦ä»»åŠ¡

### âœ… 1. TTS åˆ‡æ¢åˆ°è±†åŒ…ï¼ˆDoubaoTTSï¼‰
- **æ–‡ä»¶ä¿®æ”¹**: `ttsreal.py`
- **å…·ä½“æ”¹åŠ¨**: 
  - DoubaoTTS ç±»ä¸­ç›´æ¥ä½¿ç”¨æä¾›çš„ APP ID: `7082366049`
  - Access Token: `1fE0k8y_gCudCL8b9CLK4YXaFANOWrcH`
- **çŠ¶æ€**: âœ… å·²å®Œæˆ

### âœ… 2. MCP æ¥å£å®ç°
- **æ–°å¢æ–‡ä»¶**: 
  - `mcp_server.py` - MCP æœåŠ¡å™¨å®ç°
  - `test_mcp.py` - æµ‹è¯•å·¥å…·
- **å®ç°çš„ä¸‰ä¸ªæ¥å£**:
  1. `POST /api/speak` - è¾“å…¥æ–‡æœ¬è®©æ•°å­—äººè¯´è¯
  2. `POST /api/interrupt` - æ‰“æ–­æ•°å­—äººè¯´è¯
  3. `GET /api/status` - è·å–è¯´è¯çŠ¶æ€
- **çŠ¶æ€**: âœ… å·²å®Œæˆ

### âœ… 3. LLM é›†æˆä¼˜åŒ–
- **æ–‡ä»¶ä¿®æ”¹**: `llm/one_api.py`
- **æ–°å¢åŠŸèƒ½**:
  - æµå¼è¾“å‡ºæ”¯æŒ (`get_answer_stream`)
  - å¼‚æ­¥è°ƒç”¨æ”¯æŒ (`get_answer_async`)
  - ä¿ç•™åŸæœ‰åŒæ­¥æ¥å£å…¼å®¹æ€§
- **çŠ¶æ€**: âœ… å·²å®Œæˆ

### âœ… 4. å‰ç«¯ç•Œé¢ç¾åŒ– & GPU ç›‘æ§
- **æ–°å¢æ–‡ä»¶**:
  - `web/dashboard_enhanced.html` - å¢å¼ºç‰ˆç•Œé¢
  - `gpu_monitor.py` - GPU ç›‘æ§æ¨¡å—
  - `app_gpu_endpoint.py` - GPU æ¥å£ä»£ç ï¼ˆéœ€æ‰‹åŠ¨æ·»åŠ åˆ° app.pyï¼‰
- **ç•Œé¢ç‰¹æ€§**:
  - ç°ä»£åŒ–å¡ç‰‡è®¾è®¡
  - å®æ—¶ GPU ä½¿ç”¨ç‡æ˜¾ç¤º
  - æ˜¾å­˜å ç”¨ç›‘æ§
  - æ¸©åº¦å’ŒåŠŸè€—æ˜¾ç¤º
  - å†å²æ›²çº¿å›¾è¡¨
  - æ¸å˜è‰²ç¾åŒ–
- **çŠ¶æ€**: âœ… å·²å®Œæˆ

### âœ… 5. æ–‡æ¡£æ›´æ–°
- **åˆ é™¤æ–‡ä»¶**:
  - `README-EN.md` (å·²åˆ é™¤)
- **æ–°å¢/æ›´æ–°æ–‡ä»¶**:
  - `README.md` - ç®€æ´çš„å®‰è£…æŒ‡å—
  - `requirements.txt` - æ›´æ–°çš„ä¾èµ–åˆ—è¡¨
  - `CHANGELOG.md` - è¯¦ç»†æ”¹åŠ¨æ–‡æ¡£
  - `start.sh` - å¿«é€Ÿå¯åŠ¨è„šæœ¬
  - `stop.sh` - åœæ­¢æœåŠ¡è„šæœ¬
- **çŠ¶æ€**: âœ… å·²å®Œæˆ

## ğŸ“‚ æ–°å¢æ–‡ä»¶æ¸…å•

```
LiveTalking/
â”œâ”€â”€ mcp_server.py              # MCP æœåŠ¡å™¨
â”œâ”€â”€ test_mcp.py                # MCP æµ‹è¯•å·¥å…·
â”œâ”€â”€ gpu_monitor.py             # GPU ç›‘æ§æ¨¡å—
â”œâ”€â”€ app_gpu_endpoint.py        # GPU æ¥å£ä»£ç ç¤ºä¾‹
â”œâ”€â”€ web/dashboard_enhanced.html # å¢å¼ºç‰ˆç•Œé¢
â”œâ”€â”€ CHANGELOG.md               # æ”¹åŠ¨æ–‡æ¡£
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md  # æœ¬æ€»ç»“æ–‡æ¡£
â”œâ”€â”€ start.sh                   # å¯åŠ¨è„šæœ¬
â””â”€â”€ stop.sh                    # åœæ­¢è„šæœ¬
```

## ğŸ”§ æ‰‹åŠ¨å®æ–½æ­¥éª¤

### æ­¥éª¤ 1: æ·»åŠ  GPU ç›‘æ§åˆ°ä¸»æœåŠ¡

åœ¨ `app.py` ä¸­æ·»åŠ ï¼š

1. **å¯¼å…¥è¯­å¥**ï¼ˆæ–‡ä»¶é¡¶éƒ¨ï¼‰:
```python
from gpu_monitor import get_gpu_status, get_gpu_status_detailed
```

2. **æ·»åŠ è·¯ç”±å¤„ç†å‡½æ•°**ï¼ˆåœ¨å…¶ä»–è·¯ç”±å‡½æ•°é™„è¿‘ï¼‰:
```python
async def gpu_status(request):
    """è¿”å› GPU ä½¿ç”¨çŠ¶æ€"""
    try:
        gpu_info = get_gpu_status()
        return web.Response(
            content_type="application/json",
            text=json.dumps(gpu_info)
        )
    except Exception as e:
        logger.exception('gpu_status exception:')
        return web.Response(
            content_type="application/json",
            text=json.dumps({"error": str(e), "gpu_usage": 0, "mem_used": 0, "mem_total": 0}),
            status=500
        )
```

3. **æ³¨å†Œè·¯ç”±**ï¼ˆçº¦ç¬¬ 403 è¡Œï¼Œåœ¨å…¶ä»–è·¯ç”±æ³¨å†Œåï¼‰:
```python
appasync.router.add_get("/gpu_status", gpu_status)
appasync.router.add_get("/gpu_status_detailed", gpu_status_detailed)
```

### æ­¥éª¤ 2: è®¾ç½®æ‰§è¡Œæƒé™

```bash
chmod +x start.sh
chmod +x stop.sh
```

### æ­¥éª¤ 3: å®‰è£…é¢å¤–ä¾èµ–

```bash
# GPU ç›‘æ§ï¼ˆå¯é€‰ï¼‰
pip install nvidia-ml-py pynvml

# å¦‚æœéœ€è¦ Chart.jsï¼ˆå‰ç«¯å·²é€šè¿‡ CDN åŠ è½½ï¼Œæ— éœ€å®‰è£…ï¼‰
```

## ğŸš€ å¿«é€Ÿå¯åŠ¨æŒ‡å—

### æ–¹æ³• 1: ä½¿ç”¨å¯åŠ¨è„šæœ¬
```bash
./start.sh
```

### æ–¹æ³• 2: æ‰‹åŠ¨å¯åŠ¨
```bash
# ç»ˆç«¯ 1 - ä¸»æœåŠ¡
python app.py --tts doubao --model musetalk --transport webrtc

# ç»ˆç«¯ 2 - MCP æœåŠ¡
python mcp_server.py

# ç»ˆç«¯ 3 - æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
python test_mcp.py
```

## ğŸŒ è®¿é—®åœ°å€

- **å¢å¼ºç‰ˆç•Œé¢**: http://localhost:8010/dashboard_enhanced.html
- **æ ‡å‡†ç•Œé¢**: http://localhost:8010/dashboard.html
- **MCP æµ‹è¯•é¡µ**: http://localhost:8011/
- **WebRTC API**: http://localhost:8010/webrtcapi.html

## ğŸ“ æµ‹è¯•ç”¨ä¾‹

### æµ‹è¯• MCP æ¥å£
```bash
# å®Œæ•´æµ‹è¯•
python test_mcp.py

# äº¤äº’å¼æµ‹è¯•
python test_mcp.py --mode interactive

# æ€§èƒ½æµ‹è¯•
python test_mcp.py --mode performance
```

### æµ‹è¯• GPU ç›‘æ§
```bash
# ç‹¬ç«‹æµ‹è¯• GPU ç›‘æ§
python gpu_monitor.py
```

### ä½¿ç”¨ curl æµ‹è¯• MCP
```bash
# è®©æ•°å­—äººè¯´è¯
curl -X POST http://localhost:8011/api/speak \
  -H "Content-Type: application/json" \
  -d '{"text": "ä½ å¥½ï¼Œæˆ‘æ˜¯æ•°å­—äºº", "use_llm": false}'

# æŸ¥è¯¢çŠ¶æ€
curl http://localhost:8011/api/status

# æ‰“æ–­è¯´è¯
curl -X POST http://localhost:8011/api/interrupt
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ¨¡å‹æ–‡ä»¶**: éœ€è¦æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ° `models/` ç›®å½•
2. **CUDA ç¯å¢ƒ**: å¦‚æœæ²¡æœ‰ GPUï¼Œç³»ç»Ÿä¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
3. **ç«¯å£å ç”¨**: é»˜è®¤ä½¿ç”¨ 8010ã€8011 ç«¯å£ï¼Œå¯åœ¨å¯åŠ¨æ—¶ä¿®æ”¹
4. **TTS é…ç½®**: è±†åŒ… TTS çš„ APP ID å’Œ Token å·²ç¡¬ç¼–ç åœ¨ `ttsreal.py`
5. **LLM é…ç½®**: OneAPI çš„å¯†é’¥å·²åŒ…å«åœ¨ `llm/one_api.py`

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: GPU ç›‘æ§æ˜¾ç¤ºæ¨¡æ‹Ÿæ•°æ®
- **åŸå› **: æœªå®‰è£… nvidia-ml-py æˆ–æ—  CUDA ç¯å¢ƒ
- **è§£å†³**: å®‰è£… `pip install nvidia-ml-py pynvml`

### é—®é¢˜ 2: MCP æœåŠ¡æ— æ³•è¿æ¥
- **åŸå› **: ä¸»æœåŠ¡æœªå¯åŠ¨æˆ–ç«¯å£è¢«å ç”¨
- **è§£å†³**: å…ˆå¯åŠ¨ä¸»æœåŠ¡ `python app.py`ï¼Œç¡®ä¿ 8010 ç«¯å£å¯ç”¨

### é—®é¢˜ 3: å‰ç«¯ç•Œé¢æ— æ³•åŠ è½½
- **åŸå› **: é™æ€æ–‡ä»¶è·¯å¾„é—®é¢˜
- **è§£å†³**: ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡ŒæœåŠ¡

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å‡å°æ‰¹å¤„ç†å¤§å°**: `--batch_size 8`ï¼ˆå¦‚æœæ˜¾å­˜ä¸è¶³ï¼‰
2. **ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹**: `--model wav2lip`ï¼ˆæ¯” musetalk å¿«ï¼‰
3. **å¯ç”¨ GPU ç¼“å­˜**: è®¾ç½®ç¯å¢ƒå˜é‡ `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512`
4. **ä½¿ç”¨æœ¬åœ° TTS**: é¿å…ç½‘ç»œå»¶è¿Ÿï¼Œä½¿ç”¨ EdgeTTS ç­‰æœ¬åœ°æœåŠ¡

## ğŸ‰ é¡¹ç›®äº®ç‚¹

1. âœ¨ **ç¾è§‚çš„ç•Œé¢**: ç°ä»£åŒ–è®¾è®¡ï¼Œæ¸å˜è‰²ä¸»é¢˜
2. ğŸ“Š **å®æ—¶ç›‘æ§**: GPU ä½¿ç”¨ç‡å®æ—¶æ˜¾ç¤º
3. ğŸ”Œ **MCP æ¥å£**: æ ‡å‡†åŒ–çš„æ§åˆ¶æ¥å£
4. ğŸš€ **å¿«é€Ÿå¯åŠ¨**: ä¸€é”®å¯åŠ¨è„šæœ¬
5. ğŸ“ **å®Œå–„æ–‡æ¡£**: è¯¦ç»†çš„ä½¿ç”¨å’Œæ•…éšœæ’æŸ¥æŒ‡å—

## ğŸ“… å®Œæˆæ—¶é—´

2024å¹´12æœˆ21æ—¥

---

**æ‰€æœ‰æ”¹åŠ¨å·²å®Œæˆå¹¶è®°å½•åœ¨æ–‡æ¡£ä¸­ã€‚é¡¹ç›®ç°åœ¨å…·æœ‰æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€æ›´å¼ºå¤§çš„åŠŸèƒ½å’Œæ›´å®Œå–„çš„æ–‡æ¡£ã€‚**
